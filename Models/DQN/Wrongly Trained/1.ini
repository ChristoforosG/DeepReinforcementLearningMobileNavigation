[environment]

; simulation environment time step
time_div = 0.1

; number of robots
num_of_robots = 1

; boolean flag for fixed or random map (1 or 0)
map_flag = 1

; boolean flag for fixed or random end point (1 or 0)
end_flag = 1

; end point. Useless if end is random
ending_point = 5.0 5.0

; boolean flag for fixed or random start point (1 or 0)
start_flag = 0

; start point. Useless if start is random
starting_point = 1.0 1.0 0.0

; state: 1 = "position", 2 = "position + pose", 3 = "position + visual sensor input", 4 = "position + pose + visual sensor input"
state = 2

; reward type for each step that no event significant happens. 'step' = -1. 'relative position' = -(|dx| + |dy|)/max(|dx|+|dy|). 'relative pose'
reward_type = relative pose

; reward for obstacle collision
collision_reward = -400

; reward for finding goal
goal_reward = 1000

; evaluation or training
run_mode = training

; simulation mode. ROS or default.
simulation_mode = default

;max steps per episode
episode_steps = 500

[agent]
; algorithm. Choose from = {DQN}
algorithm = DQN

;state of the agent.{position, pose, sensor, number of other robots
state_space = position pose

; action space of the agent. Fill the parameters.
action_space = action_type= default velocity_x= [ -1.0 , 1.0 ] velocity_y_or_z= [ -1.0 , 1.0 ] discrete_actions= [ 1.0 , 0.0 ] - [ 0.0 , -0.8 ] - [ 0.0 , 0.8 ]

;neural input.Position of robot(R0) - Position of other robots(Ri) - Sensor - Goal
input = R0 Goal

;neural output
output = []

; learning rate
learning_rate = 0.001

; epsilon
epsilon = 1.0

; epsilon_decay
epsilon_decay = 0.9999

; epsilon minimum
epsilon_min = 0.01

;gamma
gamma = 0.95

;neural network 1.Main NN.
neural_network1 = 48 relu 48 relu linear mse

;batch size for DQN experiance replay sampling
batch_size = 200

[surveillance]

